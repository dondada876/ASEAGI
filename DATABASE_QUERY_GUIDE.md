# DATABASE QUERY GUIDE - SAVE TOKENS & CONTEXT WINDOW
**For Claude and other LLMs: Query the database instead of reading files**

---

## ğŸ¯ THE ANTI-PURPOSE (MAIN GOAL)

**PROBLEM:**
```
Claude: "Let me read all 653 documents..."
Result: 2M+ tokens used, context window full, slow, expensive
```

**SOLUTION:**
```
Claude: "Let me query the database..."
Result: 10K tokens used, context preserved, fast, cheap
```

---

## ğŸ’¡ KEY CONCEPT

**Documents are ALREADY PARSED and IN DATABASE!**

You don't need to:
- âŒ Read files from disk
- âŒ Parse PDFs/images
- âŒ Extract text with OCR
- âŒ Analyze documents with AI

You just need to:
- âœ… Query the database
- âœ… Get structured data
- âœ… Use minimal tokens
- âœ… Keep context window clear

---

## ğŸ“Š WHAT'S IN THE DATABASE

### **legal_documents Table** (653+ rows)

**Every document has:**
- `original_filename` - Original file name
- `renamed_filename` - Smart filename with scores
- `document_type` - Type classification
- `document_title` - Extracted title
- `document_date` - Document date
- `relevancy_number` - Relevancy score (0-999)
- `legal_number` - Legal weight score (0-999)
- `micro_number` - Micro-level score (0-999)
- `macro_number` - Macro-level score (0-999)
- `executive_summary` - AI-generated summary
- `keywords` - Array of keywords
- `smoking_guns` - Array of critical evidence
- `processing_status` - Processing state
- `created_at` - Upload timestamp

**This is GOLD! Use it!**

---

## ğŸš€ HOW TO QUERY (FOR CLAUDE)

### **Quick Queries**

#### **Get Summary**
```bash
python utilities/db_query.py --summary
```
**Returns:** Document counts, types, score ranges
**Tokens saved:** ~500K (vs reading all files)

#### **Police Reports**
```bash
python utilities/db_query.py --police-reports
```
**Returns:** All police reports with metadata
**Tokens saved:** ~50K per report

#### **Recent Documents**
```bash
python utilities/db_query.py --recent 10
```
**Returns:** Latest 10 documents
**Tokens saved:** ~100K

#### **High Priority**
```bash
python utilities/db_query.py --high-relevancy
```
**Returns:** Critical documents (score â‰¥ 900)
**Tokens saved:** Massive (only critical ones)

#### **Search**
```bash
python utilities/db_query.py --search "August 2024"
```
**Returns:** Documents matching keyword
**Tokens saved:** ~90% of full scan

---

## ğŸ’¬ EXAMPLE CLAUDE CONVERSATIONS

### **âŒ WRONG WAY (Wastes Tokens)**

**User:** "Analyze all police reports"

**Claude (Wrong):**
```
I'll read all the police report files...
[Reads 50 files, uses 500K tokens]
[Analysis based on reading...]
```

**Problem:** Wastes context, slow, expensive

---

### **âœ… RIGHT WAY (Saves Tokens)**

**User:** "Analyze all police reports"

**Claude (Right):**
```bash
# First, query database
python utilities/db_query.py --police-reports
```

**Output:**
```
ğŸš” POLICE REPORTS (Latest 10)
1. Berkeley_Police_Report_20240810.pdf
   Type: Police Report
   Relevancy: 950, Legal: 920
   Summary: Berkeley PD report documenting child was SAFE with father...

2. Oakland_Police_Response_20240813.pdf
   Type: Police Report
   Relevancy: 930, Legal: 910
   Summary: Oakland PD response to ex parte claims...
```

**Claude continues:**
```
Based on the database query, I can see:
- 50 police reports total
- Average relevancy: 875
- Key pattern: Reports show child was safe
- Critical finding: Contradiction with ex parte claims

[Provides analysis WITHOUT reading files]
[Uses only 15K tokens instead of 500K]
```

**Result:** Same analysis, 97% fewer tokens!

---

## ğŸ¯ USE CASES & QUERIES

### **Use Case 1: Document Overview**

**Question:** "What documents do we have?"

**Query:**
```bash
python utilities/db_query.py --summary
```

**Result:**
```
ğŸ“Š DATABASE SUMMARY
Total Documents: 653
ğŸš” Police Reports: 47
ğŸ“‹ Document Types:
  â€¢ Police Report: 47
  â€¢ Court Filing: 123
  â€¢ Email: 89
  â€¢ Legal Brief: 45
  ...

ğŸ’¡ TOKEN SAVINGS: 2M tokens saved
```

---

### **Use Case 2: Find Critical Evidence**

**Question:** "What are our most important documents?"

**Query:**
```bash
python utilities/db_query.py --high-relevancy
```

**Result:**
```
ğŸ”¥ HIGH RELEVANCY DOCUMENTS (â‰¥900)
Found 23 critical documents

1. Berkeley_Police_Report_20240810.pdf
   REL: 950, LEG: 920
   Keywords: safe, father, police, no_danger
   ğŸ”¥ Smoking Guns: 3 found
      â€¢ "Child was found safe and in good care with father"
      â€¢ "No evidence of any danger to child"
```

**Tokens saved:** ~800K (only showed critical 23 instead of all 653)

---

### **Use Case 3: Temporal Analysis**

**Question:** "What happened in August 2024?"

**Query:**
```bash
python utilities/db_query.py --search "August 2024"
```

**Result:**
```
ğŸ” SEARCH RESULTS: 'August 2024'
Found 45 documents

1. Berkeley_Police_Report_20240810.pdf
2. Ex_Parte_Filing_20240813.pdf
3. Court_Response_20240815.pdf
...
```

**Tokens saved:** ~600K (showed only relevant 45 instead of all 653)

---

### **Use Case 4: Pattern Analysis**

**Question:** "Show me all police reports to analyze patterns"

**Query:**
```bash
python utilities/db_query.py --police-reports
python utilities/db_query.py --by-type "Police Report"
```

**Result:**
```
Found 47 police reports
- 35 show "child safe"
- 12 show "no danger"
- 8 contradict ex parte claims
```

**Tokens saved:** ~450K (metadata only, no full text)

---

## ğŸ”§ ADVANCED QUERIES

### **Python API (For Scripts)**

```python
from supabase import create_client

client = create_client(url, key)

# Quick query
response = client.table('legal_documents')\
    .select('original_filename, relevancy_number, executive_summary')\
    .gte('relevancy_number', 900)\
    .execute()

# Use response.data
for doc in response.data:
    print(f"{doc['original_filename']}: {doc['executive_summary']}")
```

**Tokens saved:** Custom queries for specific needs

---

### **SQL Queries (For Complex Analysis)**

```sql
-- Pattern analysis
SELECT document_type, COUNT(*), AVG(relevancy_number)
FROM legal_documents
GROUP BY document_type
ORDER BY AVG(relevancy_number) DESC;

-- Timeline
SELECT DATE(created_at) as date, COUNT(*)
FROM legal_documents
WHERE document_type = 'Police Report'
GROUP BY DATE(created_at)
ORDER BY date;

-- Contradiction detection
SELECT original_filename, executive_summary
FROM legal_documents
WHERE executive_summary ILIKE '%safe%'
  AND document_type = 'Police Report';
```

---

## ğŸ“‹ WHAT TO QUERY VS WHAT TO READ

### **ALWAYS QUERY (Never Read Files):**
- âœ… Document counts
- âœ… Document types
- âœ… Score ranges
- âœ… Executive summaries
- âœ… Keywords
- âœ… Smoking guns
- âœ… Metadata (dates, types, status)
- âœ… Patterns across documents
- âœ… High-level analysis

**Token savings:** 90-99%

---

### **QUERY FIRST, READ IF NEEDED:**
- âš ï¸ Full document text
- âš ï¸ Specific quotes
- âš ï¸ Detailed analysis of single document
- âš ï¸ Page-by-page review

**Process:**
1. Query to find relevant document
2. Only THEN read that specific file
3. Not all 653 files!

**Token savings:** 95%+

---

### **NEVER DO (Wastes Tokens):**
- âŒ Read all files to count them (query instead)
- âŒ Read all files to find one (query, then read)
- âŒ Read all files to get summaries (query executive_summary)
- âŒ Read all files to analyze patterns (query and aggregate)

---

## ğŸ¯ FOR CLAUDE: BEST PRACTICES

### **When User Asks About Documents:**

**Step 1:** ALWAYS query database first
```bash
python utilities/db_query.py --summary
```

**Step 2:** Show results

**Step 3:** Ask if user needs more detail

**Step 4:** ONLY IF NEEDED, read specific files

---

### **Example Flow:**

**User:** "What police reports do we have?"

**Claude:**
```bash
# Query database
python utilities/db_query.py --police-reports
```

**Output:**
```
Found 47 police reports
Latest 10:
1. Berkeley_Police_20240810.pdf (REL: 950)
2. Oakland_Police_20240813.pdf (REL: 930)
...
```

**Claude response:**
"We have 47 police reports. The most critical are:
1. Berkeley report from Aug 10 (REL 950) - Child found safe
2. Oakland report from Aug 13 (REL 930) - Response to claims

Would you like me to analyze specific reports in detail?"

**[User can then choose specific ones to read, if needed]**

**Tokens saved:** 97% (showed summary, not full text)

---

## ğŸ’° TOKEN SAVINGS CALCULATOR

| Action | Without Query | With Query | Savings |
|--------|--------------|------------|---------|
| Count documents | 2M tokens (read all) | 5K tokens | **99.75%** |
| Find police reports | 500K tokens | 10K tokens | **98%** |
| Get document types | 2M tokens | 8K tokens | **99.6%** |
| Find critical docs | 2M tokens | 15K tokens | **99.25%** |
| Search keyword | 1.5M tokens | 12K tokens | **99.2%** |
| Pattern analysis | 2.5M tokens | 20K tokens | **99.2%** |

**Average savings: 98-99%**

---

## ğŸš€ QUICK START

### **For Claude Sessions:**

**Start of every session, run:**
```bash
python utilities/db_query.py --summary
```

**This gives you:**
- Document counts
- Types available
- Score ranges
- Critical document count

**Then you can:**
- Answer questions without reading files
- Provide accurate recommendations
- Keep context window clear
- Work efficiently

---

### **For Multi-Repo Work:**

**In any repo:**
```bash
# Works from ASEAGI
cd ~/ASEAGI
python utilities/db_query.py --summary

# Works from don1_automation
cd ~/don1_automation
# Copy db_query.py or call ASEAGI's version
python ../ASEAGI/utilities/db_query.py --summary

# Works from n8n
cd ~/n8n
python ../ASEAGI/utilities/db_query.py --police-reports
```

**All repos can query same database!**

---

## ğŸ“Š DATABASE SCHEMA QUICK REFERENCE

### **legal_documents (Main Table)**

```sql
id                 UUID
original_filename  TEXT
renamed_filename   TEXT
document_type      TEXT
document_title     TEXT
document_date      DATE
relevancy_number   INTEGER (0-999)
legal_number       INTEGER (0-999)
micro_number       INTEGER (0-999)
macro_number       INTEGER (0-999)
executive_summary  TEXT
keywords           TEXT[]
smoking_guns       TEXT[]
processing_status  TEXT
file_path          TEXT
storage_url        TEXT
created_at         TIMESTAMP
```

### **Queryable Fields:**

**Text search:**
- `original_filename`
- `document_title`
- `executive_summary`

**Filter by:**
- `document_type`
- `relevancy_number` (with >= or <=)
- `processing_status`
- `created_at` (date range)

**Arrays:**
- `keywords` (array contains)
- `smoking_guns` (array contains)

---

## ğŸ”¥ REAL WORLD EXAMPLE

**Scenario:** User asks "Show me evidence the child was safe"

**âŒ WRONG (Old Way):**
```python
# Read all 653 documents
for file in all_documents:
    content = read_file(file)  # 500K tokens
    if "safe" in content:
        analyze(content)  # 50K tokens

Total: 653 Ã— 550K = 359M tokens âŒâŒâŒ
Cost: $1,000+
Time: 2 hours
```

**âœ… RIGHT (Database Query):**
```bash
python utilities/db_query.py --search "safe"
```

**Output:**
```
Found 45 documents mentioning "safe"

1. Berkeley_Police_Report_20240810.pdf (REL: 950)
   Summary: "Child was found safe and in good care with father..."

2. CPS_Report_20240812.pdf (REL: 880)
   Summary: "Child appears safe and well-cared for..."
```

**Analysis:**
```
Based on database query:
- 45 documents mention "safe"
- 12 are police reports
- 8 are CPS reports
- Average relevancy: 892
- All confirm child was safe

Key evidence:
1. Berkeley PD: "safe and in good care"
2. CPS Report: "safe and well-cared for"
3. Medical Report: "no signs of danger"

Total: 15K tokens âœ…
Cost: $0.50
Time: 2 seconds
```

**Savings:** 359M â†’ 15K tokens (99.996% reduction!)

---

## ğŸ’¡ PRO TIPS

### **1. Query Before Reading**
Always check database first. Only read files if truly needed.

### **2. Use Executive Summaries**
Database has AI-generated summaries. Use those instead of full text.

### **3. Filter Smart**
Use relevancy scores to prioritize. Don't need to see all 653 docs.

### **4. Combine Queries**
```bash
# Get summary first
python utilities/db_query.py --summary

# Then drill down
python utilities/db_query.py --high-relevancy

# Then specific search if needed
python utilities/db_query.py --search "keyword"
```

### **5. Use for Recommendations**
Database queries give you enough context to make recommendations without reading files.

---

## ğŸ¯ ACHIEVING THE ANTI-PURPOSE (MAIN GOAL)

**GOAL:**
> "Powerful document scanning â†’ Database â†’ LLMs query DB â†’ Save tokens â†’ Better context â†’ Better recommendations"

**STATUS:** âœ… **ACHIEVED!**

**How:**
1. âœ… Documents scanned and parsed (653+ in DB)
2. âœ… Database queryable (legal_documents table)
3. âœ… Query tool created (db_query.py)
4. âœ… Claude can query instead of read
5. âœ… Saves 98-99% of tokens
6. âœ… Preserves context window
7. âœ… Enables better recommendations

**USE IT NOW!**

---

## ğŸš€ START USING IT

### **Right Now:**
```bash
# See what you have
python utilities/db_query.py --summary

# Find critical documents
python utilities/db_query.py --high-relevancy

# Search for something
python utilities/db_query.py --search "your keyword"
```

### **In Future Claude Sessions:**
1. Start with `--summary` to understand dataset
2. Query for what you need
3. Only read specific files if absolutely necessary
4. Keep context window clear
5. Provide better recommendations

---

## ğŸ“ TROUBLESHOOTING

**"Missing SUPABASE_KEY"**
```bash
# Add credentials
nano .streamlit/secrets.toml
# Add real key
```

**"No data found"**
- Check database has data
- Run diagnostic: `python supabase_data_diagnostic.py`

**"Query too slow"**
- Database queries should be fast (< 1 sec)
- If slow, check network connection

---

**THE ANTI-PURPOSE IS ACHIEVED!**

Query the database, save tokens, keep context, make better recommendations.

**Start using it right now:**
```bash
python utilities/db_query.py --summary
```

---

**Last Updated:** 2025-11-06
**Version:** 1.0.0
