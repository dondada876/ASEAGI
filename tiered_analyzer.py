#!/usr/bin/env python3
"""
ASEAGI Tiered Analysis Service
Multi-tier legal document analysis system

TIER 1: MICRO ANALYSIS (per document)
TIER 2: MACRO ANALYSIS (cross-document)
TIER 3: VIOLATION ANALYSIS
TIER 4: CASE LAW & CITATIONS
TIER 5: EVENT TIMELINE & PROFILES
TIER 6: JUDICIAL ASSESSMENT
"""

import os
import json
from dataclasses import dataclass, asdict
from typing import List, Dict, Optional, Any
from datetime import datetime, date
import re

try:
    from supabase import create_client
except ImportError:
    print("[ERROR] Supabase not installed")
    print("Run: pip install supabase")
    exit(1)

try:
    from openai import OpenAI
except ImportError:
    print("[ERROR] OpenAI not installed")
    print("Run: pip install openai")
    exit(1)


# ============================================================================
# DATA CLASSES
# ============================================================================

@dataclass
class MicroAnalysisResult:
    """Result of Tier 1 micro analysis"""
    micro_id: Optional[int]
    journal_id: int
    document_type: str
    critical_statements: Dict[str, Any]
    entities: Dict[str, Any]
    dates_mentioned: Dict[str, Any]
    claims: List[Dict[str, Any]]
    facts: List[Dict[str, Any]]
    extraction_confidence: float
    ready_for_macro: bool = True
    needs_manual_review: bool = False


@dataclass
class MacroAnalysisResult:
    """Result of Tier 2 macro analysis"""
    macro_id: Optional[int]
    analysis_name: str
    analysis_type: str
    documents_analyzed: List[int]
    findings: Dict[str, Any]
    consistency_score: float
    reliability_score: float
    cross_references: List[Dict[str, Any]]
    patterns: List[Dict[str, Any]]
    legal_relevancy_score: float
    potential_violations: bool = False


@dataclass
class ViolationResult:
    """Result of Tier 3 violation analysis"""
    violation_id: Optional[int]
    violation_type: str
    violation_severity: str
    violator_name: str
    violation_date: date
    violated_law_or_order: str
    evidence_documents: List[int]
    false_statements: List[Dict[str, Any]]
    confidence_score: float
    recommended_action: str


# ============================================================================
# TIERED ANALYZER
# ============================================================================

class TieredAnalyzer:
    """Multi-tier legal document analysis service"""

    def __init__(self, supabase_url: str, supabase_key: str, openai_key: str):
        self.supabase = create_client(supabase_url, supabase_key)
        self.openai = OpenAI(api_key=openai_key)

    # ========================================================================
    # TIER 1: MICRO ANALYSIS (Per Document)
    # ========================================================================

    def micro_analyze_document(self, journal_id: int) -> MicroAnalysisResult:
        """
        Run Tier 1 micro analysis on a single document

        Extracts:
        - Critical statements
        - Entities (people, agencies, locations)
        - Dates mentioned
        - Claims vs Facts
        - Key data fields
        """

        print(f"\n{'='*80}")
        print(f"TIER 1: MICRO ANALYSIS - Document {journal_id}")
        print(f"{'='*80}\n")

        # Get document from journal
        doc = self._get_document(journal_id)
        if not doc:
            raise ValueError(f"Document {journal_id} not found")

        document_type = doc.get('document_type', 'unknown')
        print(f"üìÑ Document type: {document_type}")

        # Get document content (from document_repository or OCR)
        content = self._get_document_content(journal_id)
        if not content:
            print("‚ö†Ô∏è  No content available for analysis")
            return MicroAnalysisResult(
                micro_id=None,
                journal_id=journal_id,
                document_type=document_type,
                critical_statements={},
                entities={},
                dates_mentioned={},
                claims=[],
                facts=[],
                extraction_confidence=0.0,
                needs_manual_review=True
            )

        # Run AI extraction based on document type
        print("ü§ñ Running AI extraction...")
        extraction = self._ai_extract_critical_info(content, document_type)

        # Store to database
        micro_id = self._save_micro_analysis(
            journal_id=journal_id,
            document_type=document_type,
            **extraction
        )

        print(f"‚úÖ Micro analysis complete: micro_id={micro_id}")
        print(f"   Entities extracted: {len(extraction['entities'].get('people', []))}")
        print(f"   Claims found: {len(extraction['claims'])}")
        print(f"   Facts found: {len(extraction['facts'])}")

        return MicroAnalysisResult(
            micro_id=micro_id,
            journal_id=journal_id,
            document_type=document_type,
            **extraction
        )

    def _ai_extract_critical_info(self, content: str, document_type: str) -> Dict[str, Any]:
        """Use AI to extract critical information based on document type"""

        # Create document-type-specific prompt
        prompt = self._create_extraction_prompt(document_type)

        try:
            response = self.openai.chat.completions.create(
                model="gpt-4",
                messages=[
                    {"role": "system", "content": prompt},
                    {"role": "user", "content": f"Analyze this document:\n\n{content}"}
                ],
                response_format={"type": "json_object"},
                temperature=0.1
            )

            result = json.loads(response.choices[0].message.content)

            return {
                'critical_statements': result.get('critical_statements', {}),
                'entities': result.get('entities', {}),
                'dates_mentioned': result.get('dates_mentioned', {}),
                'claims': result.get('claims', []),
                'facts': result.get('facts', []),
                'extraction_confidence': result.get('confidence', 0.75),
                'ready_for_macro': True,
                'needs_manual_review': False
            }

        except Exception as e:
            print(f"‚ö†Ô∏è  AI extraction failed: {e}")
            return {
                'critical_statements': {},
                'entities': {},
                'dates_mentioned': {},
                'claims': [],
                'facts': [],
                'extraction_confidence': 0.0,
                'ready_for_macro': False,
                'needs_manual_review': True
            }

    def _create_extraction_prompt(self, document_type: str) -> str:
        """Create document-type-specific extraction prompt"""

        base_prompt = """You are a legal document analyzer. Extract critical information in JSON format.

Return a JSON object with these fields:
- critical_statements: Key statements/claims made in the document
- entities: People, agencies, locations mentioned
- dates_mentioned: All dates referenced (incident dates, court dates, deadlines)
- claims: Allegations or claims made (not yet verified)
- facts: Verifiable facts stated
- confidence: Your confidence in the extraction (0-1)

"""

        type_specific = {
            'police_report': """
For a police report, extract:
- Officer name, badge number
- Incident date, time, location
- Statements from all parties
- Allegations made
- Disposition (founded/unfounded)
- Any evidence collected
""",
            'court_filing': """
For a court filing (ex parte, motion, declaration), extract:
- Filing party
- Date filed
- Claims made
- Relief requested
- Evidence cited
- Sworn statements
- Declarant information
""",
            'medical_report': """
For a medical report, extract:
- Doctor/provider name
- Facility
- Visit date
- Diagnosis
- Findings/observations
- Injuries (if any)
- Recommendations
- Treatment provided
""",
            'declaration': """
For a declaration, extract:
- Declarant name and role
- Date sworn
- All claims made
- Specific incidents described
- Dates of alleged incidents
- Other parties mentioned
- Evidence referenced
""",
            'default': """
Extract all critical information, statements, entities, and dates.
"""
        }

        return base_prompt + type_specific.get(document_type, type_specific['default'])

    # ========================================================================
    # TIER 2: MACRO ANALYSIS (Cross-Document)
    # ========================================================================

    def macro_analyze_cross_reference(
        self,
        document_ids: List[int],
        analysis_type: str = 'consistency_check'
    ) -> MacroAnalysisResult:
        """
        Run Tier 2 macro analysis across multiple documents

        Analysis types:
        - consistency_check: Check for contradictions
        - ex_parte_verification: Verify ex parte claims against evidence
        - timeline_verification: Verify event timeline
        - statement_comparison: Compare statements across documents
        """

        print(f"\n{'='*80}")
        print(f"TIER 2: MACRO ANALYSIS - {analysis_type}")
        print(f"{'='*80}\n")
        print(f"üìä Analyzing {len(document_ids)} documents")

        # Get micro analyses for all documents
        micro_analyses = []
        for doc_id in document_ids:
            micro = self._get_micro_analysis(doc_id)
            if micro:
                micro_analyses.append(micro)

        if len(micro_analyses) < 2:
            print("‚ö†Ô∏è  Need at least 2 documents for macro analysis")
            return None

        print(f"‚úÖ Retrieved {len(micro_analyses)} micro analyses")

        # Run cross-reference analysis
        print(f"üîç Running {analysis_type}...")

        if analysis_type == 'consistency_check':
            result = self._check_consistency(micro_analyses)
        elif analysis_type == 'ex_parte_verification':
            result = self._verify_ex_parte(micro_analyses)
        elif analysis_type == 'statement_comparison':
            result = self._compare_statements(micro_analyses)
        else:
            result = self._generic_cross_reference(micro_analyses)

        # Store to database
        macro_id = self._save_macro_analysis(
            analysis_type=analysis_type,
            document_ids=document_ids,
            **result
        )

        print(f"‚úÖ Macro analysis complete: macro_id={macro_id}")
        print(f"   Consistency score: {result['consistency_score']:.1f}%")
        print(f"   Cross-references found: {len(result['cross_references'])}")
        print(f"   Potential violations: {result['potential_violations']}")

        return MacroAnalysisResult(
            macro_id=macro_id,
            analysis_name=f"{analysis_type}_{datetime.now().strftime('%Y%m%d')}",
            analysis_type=analysis_type,
            documents_analyzed=document_ids,
            **result
        )

    def _check_consistency(self, micro_analyses: List[Dict]) -> Dict[str, Any]:
        """Check consistency across documents"""

        # Extract all claims from all documents
        all_claims = []
        for micro in micro_analyses:
            claims = micro.get('claims', [])
            for claim in claims:
                all_claims.append({
                    'claim': claim,
                    'document_id': micro['journal_id'],
                    'document_type': micro['document_type']
                })

        # Use AI to find contradictions
        contradictions = self._ai_find_contradictions(all_claims)

        # Calculate consistency score
        total_claims = len(all_claims)
        contradicted_claims = len(contradictions)
        consistency_score = ((total_claims - contradicted_claims) / total_claims * 100) if total_claims > 0 else 100

        return {
            'findings': {
                'total_claims': total_claims,
                'contradictions': contradictions,
                'consistency_issues': len(contradictions)
            },
            'consistency_score': consistency_score,
            'reliability_score': 100 - (len(contradictions) * 5),  # Deduct 5 points per contradiction
            'cross_references': contradictions,
            'patterns': [],
            'legal_relevancy_score': 80.0,  # Will be refined later
            'potential_violations': len(contradictions) > 0
        }

    def _verify_ex_parte(self, micro_analyses: List[Dict]) -> Dict[str, Any]:
        """Verify ex parte filing against supporting evidence"""

        # Find the ex parte document
        ex_parte = None
        evidence_docs = []

        for micro in micro_analyses:
            if 'ex_parte' in micro.get('document_type', '').lower():
                ex_parte = micro
            else:
                evidence_docs.append(micro)

        if not ex_parte:
            return {
                'findings': {'error': 'No ex parte document found'},
                'consistency_score': 0,
                'reliability_score': 0,
                'cross_references': [],
                'patterns': [],
                'legal_relevancy_score': 0,
                'potential_violations': False
            }

        # Extract claims from ex parte
        ex_parte_claims = ex_parte.get('critical_statements', {}).get('claims', [])

        # Check if each claim is supported by evidence
        verified_claims = []
        unsupported_claims = []

        for claim in ex_parte_claims:
            is_supported = self._is_claim_supported(claim, evidence_docs)
            if is_supported:
                verified_claims.append(claim)
            else:
                unsupported_claims.append(claim)

        # Calculate fraud likelihood
        total_claims = len(ex_parte_claims)
        unsupported_count = len(unsupported_claims)
        fraud_likelihood = (unsupported_count / total_claims) if total_claims > 0 else 0

        return {
            'findings': {
                'type': 'ex_parte_verification',
                'ex_parte_journal_id': ex_parte['journal_id'],
                'total_claims': total_claims,
                'verified_claims': len(verified_claims),
                'unsupported_claims': unsupported_count,
                'fraudulent_filing_likelihood': fraud_likelihood
            },
            'consistency_score': (len(verified_claims) / total_claims * 100) if total_claims > 0 else 0,
            'reliability_score': 100 - (unsupported_count * 20),
            'cross_references': [
                {
                    'claim': claim,
                    'supported': False,
                    'evidence': 'none'
                }
                for claim in unsupported_claims
            ],
            'patterns': [],
            'legal_relevancy_score': 100 if fraud_likelihood > 0.5 else 50,
            'potential_violations': fraud_likelihood > 0.3
        }

    def _compare_statements(self, micro_analyses: List[Dict]) -> Dict[str, Any]:
        """Compare statements across multiple documents"""

        # Extract all statements
        all_statements = []
        for micro in micro_analyses:
            statements = micro.get('critical_statements', {}).get('statements', [])
            for stmt in statements:
                all_statements.append({
                    'statement': stmt,
                    'document_id': micro['journal_id'],
                    'document_type': micro['document_type'],
                    'source': micro.get('entities', {}).get('people', [{}])[0].get('name', 'Unknown')
                })

        # Group by speaker and compare
        by_speaker = {}
        for stmt in all_statements:
            speaker = stmt['source']
            if speaker not in by_speaker:
                by_speaker[speaker] = []
            by_speaker[speaker].append(stmt)

        # Find contradictions per speaker
        contradictions = []
        for speaker, statements in by_speaker.items():
            if len(statements) >= 2:
                speaker_contradictions = self._find_speaker_contradictions(statements)
                contradictions.extend(speaker_contradictions)

        consistency_score = 100 - (len(contradictions) * 10)

        return {
            'findings': {
                'speakers_analyzed': len(by_speaker),
                'total_statements': len(all_statements),
                'contradictions_found': len(contradictions)
            },
            'consistency_score': max(0, consistency_score),
            'reliability_score': 85,
            'cross_references': contradictions,
            'patterns': [],
            'legal_relevancy_score': 75,
            'potential_violations': len(contradictions) > 2
        }

    # ========================================================================
    # TIER 3: VIOLATION ANALYSIS
    # ========================================================================

    def detect_violations(self, macro_analysis_id: int) -> List[ViolationResult]:
        """
        Detect legal violations from macro analysis

        Violation types:
        - perjury (false statements under oath)
        - fraud_upon_court (ex parte with false info)
        - false_allegations
        - protective_order_violation
        - child_endangerment
        """

        print(f"\n{'='*80}")
        print(f"TIER 3: VIOLATION ANALYSIS - Macro {macro_analysis_id}")
        print(f"{'='*80}\n")

        # Get macro analysis
        macro = self._get_macro_analysis(macro_analysis_id)
        if not macro:
            print("‚ö†Ô∏è  Macro analysis not found")
            return []

        findings = macro.get('findings', {})
        analysis_type = macro.get('analysis_type', '')

        violations = []

        # Detect perjury (false statements under oath)
        if analysis_type == 'consistency_check':
            contradictions = findings.get('contradictions', [])
            for contradiction in contradictions:
                # Check if statement was sworn
                if self._is_sworn_statement(contradiction):
                    violation = self._create_perjury_violation(
                        macro_analysis_id=macro_analysis_id,
                        contradiction=contradiction
                    )
                    violations.append(violation)

        # Detect fraud upon court
        elif analysis_type == 'ex_parte_verification':
            fraud_likelihood = findings.get('fraudulent_filing_likelihood', 0)
            if fraud_likelihood > 0.3:
                violation = self._create_fraud_violation(
                    macro_analysis_id=macro_analysis_id,
                    findings=findings
                )
                violations.append(violation)

        # Detect false allegations pattern
        if len(violations) >= 3:
            pattern_violation = self._create_pattern_violation(
                macro_analysis_id=macro_analysis_id,
                violations=violations
            )
            violations.append(pattern_violation)

        print(f"‚úÖ Detected {len(violations)} violations")
        for v in violations:
            print(f"   - {v.violation_type}: {v.violator_name} ({v.violation_severity})")

        return violations

    def _create_perjury_violation(
        self,
        macro_analysis_id: int,
        contradiction: Dict
    ) -> ViolationResult:
        """Create perjury violation record"""

        return ViolationResult(
            violation_id=None,
            violation_type='perjury',
            violation_severity='severe',
            violator_name=contradiction.get('violator', 'Unknown'),
            violation_date=datetime.now().date(),
            violated_law_or_order='California Penal Code ¬ß 118 (Perjury)',
            evidence_documents=contradiction.get('evidence_journal_ids', []),
            false_statements=[contradiction],
            confidence_score=85.0,
            recommended_action='Criminal investigation for perjury'
        )

    def _create_fraud_violation(
        self,
        macro_analysis_id: int,
        findings: Dict
    ) -> ViolationResult:
        """Create fraud upon court violation"""

        return ViolationResult(
            violation_id=None,
            violation_type='fraud_upon_court',
            violation_severity='severe',
            violator_name=findings.get('filing_party', 'Unknown'),
            violation_date=datetime.now().date(),
            violated_law_or_order='Fraud upon the Court',
            evidence_documents=[findings.get('ex_parte_journal_id')],
            false_statements=findings.get('unsupported_claims', []),
            confidence_score=findings.get('fraudulent_filing_likelihood', 0) * 100,
            recommended_action='Sanctions, costs, and referral for investigation'
        )

    # ========================================================================
    # TIER 5: EVENT TIMELINE & PROFILES
    # ========================================================================

    def build_timeline(self, journal_ids: List[int]) -> List[Dict]:
        """Build comprehensive event timeline from documents"""

        print(f"\n{'='*80}")
        print(f"TIER 5: EVENT TIMELINE BUILDING")
        print(f"{'='*80}\n")

        events = []

        for journal_id in journal_ids:
            micro = self._get_micro_analysis(journal_id)
            if not micro:
                continue

            # Extract events from micro analysis
            doc_events = self._extract_events_from_micro(micro)
            events.extend(doc_events)

        # Sort by date
        events.sort(key=lambda e: e.get('event_date', '9999-12-31'))

        # Store to database
        for event in events:
            event_id = self._save_event(event)
            event['event_id'] = event_id

        print(f"‚úÖ Built timeline with {len(events)} events")

        return events

    def build_profiles(self, journal_ids: List[int]) -> Dict[str, Dict]:
        """Build profiles for all parties mentioned in documents"""

        print(f"\n{'='*80}")
        print(f"TIER 5: PROFILE BUILDING")
        print(f"{'='*80}\n")

        # Collect all mentions of people
        people_data = {}

        for journal_id in journal_ids:
            micro = self._get_micro_analysis(journal_id)
            if not micro:
                continue

            entities = micro.get('entities', {})
            people = entities.get('people', [])

            for person in people:
                name = person.get('name')
                if not name:
                    continue

                if name not in people_data:
                    people_data[name] = {
                        'name': name,
                        'role': person.get('role', 'unknown'),
                        'statements': [],
                        'documents_mentioned_in': [],
                        'events': []
                    }

                people_data[name]['documents_mentioned_in'].append(journal_id)

                # Add statements
                statements = micro.get('critical_statements', {}).get('statements', [])
                for stmt in statements:
                    if stmt.get('speaker') == name:
                        people_data[name]['statements'].append({
                            'statement': stmt.get('statement'),
                            'document_id': journal_id,
                            'date': micro.get('document_date')
                        })

        # Calculate credibility scores
        for name, data in people_data.items():
            credibility = self._calculate_credibility(data)
            data['credibility_score'] = credibility

            # Store profile
            profile_id = self._save_profile(data)
            data['profile_id'] = profile_id

        print(f"‚úÖ Built profiles for {len(people_data)} people")

        return people_data

    # ========================================================================
    # HELPER METHODS
    # ========================================================================

    def _get_document(self, journal_id: int) -> Optional[Dict]:
        """Get document from journal"""
        result = self.supabase.table('document_journal')\
            .select('*')\
            .eq('journal_id', journal_id)\
            .execute()
        return result.data[0] if result.data else None

    def _get_document_content(self, journal_id: int) -> Optional[str]:
        """Get document content from repository"""
        # Try document_repository first
        result = self.supabase.table('document_repository')\
            .select('full_text')\
            .eq('id', journal_id)\
            .execute()

        if result.data and result.data[0].get('full_text'):
            return result.data[0]['full_text']

        # Fallback to OCR text from journal
        doc = self._get_document(journal_id)
        return doc.get('extracted_metadata', {}).get('ocr_text', '') if doc else None

    def _get_micro_analysis(self, journal_id: int) -> Optional[Dict]:
        """Get micro analysis for document"""
        result = self.supabase.table('micro_analysis')\
            .select('*')\
            .eq('journal_id', journal_id)\
            .execute()
        return result.data[0] if result.data else None

    def _get_macro_analysis(self, macro_id: int) -> Optional[Dict]:
        """Get macro analysis"""
        result = self.supabase.table('macro_analysis')\
            .select('*')\
            .eq('macro_id', macro_id)\
            .execute()
        return result.data[0] if result.data else None

    def _save_micro_analysis(self, **kwargs) -> int:
        """Save micro analysis to database"""
        result = self.supabase.table('micro_analysis')\
            .insert(kwargs)\
            .execute()
        return result.data[0]['micro_id']

    def _save_macro_analysis(self, **kwargs) -> int:
        """Save macro analysis to database"""
        result = self.supabase.table('macro_analysis')\
            .insert(kwargs)\
            .execute()
        return result.data[0]['macro_id']

    def _save_event(self, event: Dict) -> int:
        """Save event to timeline"""
        result = self.supabase.table('events')\
            .insert(event)\
            .execute()
        return result.data[0]['event_id']

    def _save_profile(self, profile: Dict) -> int:
        """Save profile to database"""
        result = self.supabase.table('profiles')\
            .insert(profile)\
            .execute()
        return result.data[0]['profile_id']

    def _ai_find_contradictions(self, claims: List[Dict]) -> List[Dict]:
        """Use AI to find contradictions across claims"""
        # Simplified - in production, use embeddings + semantic search
        contradictions = []

        for i, claim1 in enumerate(claims):
            for claim2 in claims[i+1:]:
                if self._claims_contradict(claim1, claim2):
                    contradictions.append({
                        'claim_1': claim1,
                        'claim_2': claim2,
                        'type': 'contradiction'
                    })

        return contradictions

    def _claims_contradict(self, claim1: Dict, claim2: Dict) -> bool:
        """Check if two claims contradict (simplified)"""
        # In production, use AI/embeddings for semantic contradiction detection
        text1 = str(claim1.get('claim', '')).lower()
        text2 = str(claim2.get('claim', '')).lower()

        # Simple heuristic: look for opposite keywords
        opposites = [
            ('denied', 'allowed'),
            ('injured', 'uninjured'),
            ('abused', 'no abuse'),
            ('dangerous', 'safe')
        ]

        for word1, word2 in opposites:
            if word1 in text1 and word2 in text2:
                return True
            if word2 in text1 and word1 in text2:
                return True

        return False

    def _is_claim_supported(self, claim: Dict, evidence_docs: List[Dict]) -> bool:
        """Check if claim is supported by evidence documents"""
        # Simplified - in production, use semantic search
        claim_text = str(claim).lower()

        for doc in evidence_docs:
            doc_text = str(doc.get('critical_statements', {})).lower()
            if claim_text in doc_text:
                return True

        return False

    def _is_sworn_statement(self, statement: Dict) -> bool:
        """Check if statement was made under oath"""
        doc_type = statement.get('document_type', '').lower()
        return 'declaration' in doc_type or 'testimony' in doc_type

    def _find_speaker_contradictions(self, statements: List[Dict]) -> List[Dict]:
        """Find contradictions within one speaker's statements"""
        contradictions = []

        for i, stmt1 in enumerate(statements):
            for stmt2 in statements[i+1:]:
                if self._claims_contradict(stmt1, stmt2):
                    contradictions.append({
                        'speaker': stmt1.get('source'),
                        'statement_1': stmt1,
                        'statement_2': stmt2,
                        'type': 'self_contradiction'
                    })

        return contradictions

    def _extract_events_from_micro(self, micro: Dict) -> List[Dict]:
        """Extract events from micro analysis"""
        events = []

        dates = micro.get('dates_mentioned', {})
        for date_type, date_list in dates.items():
            for date_str in date_list:
                events.append({
                    'event_date': date_str,
                    'event_type': date_type,
                    'event_title': f"{date_type} - {micro.get('document_type')}",
                    'source_documents': [micro['journal_id']],
                    'significance_score': 50.0
                })

        return events

    def _calculate_credibility(self, person_data: Dict) -> float:
        """Calculate credibility score for a person"""
        # Simplified credibility calculation
        # In production, factor in:
        # - Number of contradicted statements
        # - Number of verified statements
        # - Consistency over time

        total_statements = len(person_data.get('statements', []))
        if total_statements == 0:
            return 50.0  # Neutral

        # Placeholder - would need violation data to compute properly
        return 75.0

    def _generic_cross_reference(self, micro_analyses: List[Dict]) -> Dict[str, Any]:
        """Generic cross-reference analysis"""
        return {
            'findings': {'type': 'generic'},
            'consistency_score': 80.0,
            'reliability_score': 80.0,
            'cross_references': [],
            'patterns': [],
            'legal_relevancy_score': 70.0,
            'potential_violations': False
        }


# ============================================================================
# CLI INTERFACE
# ============================================================================

if __name__ == "__main__":
    import sys

    print("="*80)
    print("ASEAGI TIERED ANALYZER")
    print("="*80)
    print()

    # Get credentials
    SUPABASE_URL = os.environ.get('SUPABASE_URL', '')
    SUPABASE_KEY = os.environ.get('SUPABASE_KEY', '')
    OPENAI_KEY = os.environ.get('OPENAI_API_KEY', '')

    if not all([SUPABASE_URL, SUPABASE_KEY, OPENAI_KEY]):
        print("‚ùå Missing credentials. Set environment variables:")
        print("   SUPABASE_URL")
        print("   SUPABASE_KEY")
        print("   OPENAI_API_KEY")
        sys.exit(1)

    # Initialize analyzer
    analyzer = TieredAnalyzer(SUPABASE_URL, SUPABASE_KEY, OPENAI_KEY)

    # Example usage
    if len(sys.argv) > 1:
        command = sys.argv[1]

        if command == 'micro' and len(sys.argv) > 2:
            journal_id = int(sys.argv[2])
            result = analyzer.micro_analyze_document(journal_id)
            print(f"\n‚úÖ Micro analysis complete: {result.micro_id}")

        elif command == 'macro' and len(sys.argv) > 2:
            journal_ids = [int(x) for x in sys.argv[2].split(',')]
            result = analyzer.macro_analyze_cross_reference(journal_ids)
            print(f"\n‚úÖ Macro analysis complete: {result.macro_id}")

        elif command == 'violations' and len(sys.argv) > 2:
            macro_id = int(sys.argv[2])
            violations = analyzer.detect_violations(macro_id)
            print(f"\n‚úÖ Detected {len(violations)} violations")

        else:
            print("Usage:")
            print("  python3 tiered_analyzer.py micro <journal_id>")
            print("  python3 tiered_analyzer.py macro <journal_id1>,<journal_id2>,...")
            print("  python3 tiered_analyzer.py violations <macro_id>")
    else:
        print("Usage:")
        print("  python3 tiered_analyzer.py micro <journal_id>")
        print("  python3 tiered_analyzer.py macro <journal_id1>,<journal_id2>,...")
        print("  python3 tiered_analyzer.py violations <macro_id>")

    print("\nFor Ashe. For Justice. For All Children. üõ°Ô∏è")
