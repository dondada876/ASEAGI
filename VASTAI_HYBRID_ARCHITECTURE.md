# ASEAGI Hybrid Cloud Architecture
## Vast.ai GPU Processing + DigitalOcean Persistent Services

**Date:** November 14, 2025
**Version:** 1.0
**Target:** Mac Mini development â†’ Production deployment

---

## ðŸŽ¯ Architecture Philosophy

### The Smart Approach: Elastic GPU + Persistent Storage

```
ðŸ’° COST OPTIMIZATION STRATEGY:

OLD WAY (Expensive):
â”œâ”€ GPU Droplet: $800/month running 24/7
â”œâ”€ Most time idle (no processing)
â””â”€ Total: $9,600/year wasted

NEW WAY (Smart):
â”œâ”€ Vast.ai GPU: $0.40/hour Ã— 4 hours/day Ã— 20 days = $32/month
â”œâ”€ Droplet (no GPU): $24/month running 24/7
â”œâ”€ Cloud Storage: $5/month (DigitalOcean Spaces)
â””â”€ Total: $61/month = $732/year (SAVES $8,868!)
```

### Key Principles

1. **Vast.ai for Processing** - Spin up only when needed
2. **Droplet for Services** - Always-on lightweight dashboard
3. **Cloud Storage for Files** - Persistent, accessible from both
4. **Supabase for Database** - Metadata and logs
5. **Ephemeral Compute** - No state on Vast.ai instances

---

## ðŸ—ï¸ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      USER INTERFACES                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                   â”‚
â”‚  ðŸ“± Telegram Bot        ðŸŒ Web Dashboard       ðŸ’» Mac Mini       â”‚
â”‚  (Mobile upload)        (Monitor & control)    (Development)      â”‚
â”‚                                                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚              â”‚              â”‚
                 â–¼              â–¼              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          DIGITALOCEAN DROPLET (Always Running - $24/mo)          â”‚
â”‚                    Ubuntu 22.04 | 2GB RAM                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                   â”‚
â”‚  ðŸŽ›ï¸ Control Plane (Flask API)                                   â”‚
â”‚     â”œâ”€ Vast.ai instance launcher                                â”‚
â”‚     â”œâ”€ Job queue manager                                        â”‚
â”‚     â”œâ”€ Status monitor                                           â”‚
â”‚     â””â”€ Telegram webhook receiver                                â”‚
â”‚                                                                   â”‚
â”‚  ðŸ“Š Monitoring Dashboard (Streamlit)                             â”‚
â”‚     â”œâ”€ Processing status                                        â”‚
â”‚     â”œâ”€ Cost tracking                                            â”‚
â”‚     â”œâ”€ Document browser                                         â”‚
â”‚     â””â”€ Vast.ai instance control                                 â”‚
â”‚                                                                   â”‚
â”‚  ðŸ—„ï¸ Lightweight Services                                         â”‚
â”‚     â”œâ”€ Redis (job queue)                                        â”‚
â”‚     â”œâ”€ Nginx (reverse proxy)                                    â”‚
â”‚     â””â”€ Certbot (SSL)                                            â”‚
â”‚                                                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚              â”‚              â”‚
                 â”‚              â”‚              â–¼
                 â”‚              â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚              â”‚      â”‚  SUPABASE DB    â”‚
                 â”‚              â”‚      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                 â”‚              â”‚      â”‚ â€¢ Metadata      â”‚
                 â”‚              â”‚      â”‚ â€¢ File paths    â”‚
                 â”‚              â”‚      â”‚ â€¢ Processing    â”‚
                 â”‚              â”‚      â”‚   logs          â”‚
                 â”‚              â”‚      â”‚ â€¢ User data     â”‚
                 â”‚              â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚              â”‚
                 â”‚              â–¼
                 â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚      â”‚  CLOUD STORAGE (DO Spaces)      â”‚
                 â”‚      â”‚  S3-Compatible - $5/mo          â”‚
                 â”‚      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                 â”‚      â”‚                                 â”‚
                 â”‚      â”‚  ðŸ“ /raw-documents/             â”‚
                 â”‚      â”‚     â””â”€ Original uploads         â”‚
                 â”‚      â”‚                                 â”‚
                 â”‚      â”‚  ðŸ“ /processed/                 â”‚
                 â”‚      â”‚     â”œâ”€ OCR text                 â”‚
                 â”‚      â”‚     â”œâ”€ Thumbnails               â”‚
                 â”‚      â”‚     â””â”€ Metadata JSON            â”‚
                 â”‚      â”‚                                 â”‚
                 â”‚      â”‚  ðŸ“ /exports/                   â”‚
                 â”‚      â”‚     â””â”€ Generated reports        â”‚
                 â”‚      â”‚                                 â”‚
                 â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚              â–²
                 â”‚              â”‚ (Mounts via s3fs)
                 â–¼              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         VAST.AI GPU INSTANCE (On-Demand - $0.40/hr)             â”‚
â”‚                RTX 3080 | 8GB VRAM | Docker                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                   â”‚
â”‚  ðŸ³ Docker Container: aseagi/document-processor:latest          â”‚
â”‚                                                                   â”‚
â”‚  ðŸ“¦ Installed:                                                   â”‚
â”‚     â”œâ”€ Python 3.11                                              â”‚
â”‚     â”œâ”€ PyTorch (GPU)                                            â”‚
â”‚     â”œâ”€ Tesseract OCR                                            â”‚
â”‚     â”œâ”€ Claude API client                                        â”‚
â”‚     â”œâ”€ Supabase client                                          â”‚
â”‚     â”œâ”€ S3 client (boto3)                                        â”‚
â”‚     â””â”€ All processing scripts                                   â”‚
â”‚                                                                   â”‚
â”‚  âš™ï¸ Processing Pipeline:                                         â”‚
â”‚     1. Mount cloud storage (read-only for raw docs)             â”‚
â”‚     2. Fetch job from Supabase queue                            â”‚
â”‚     3. Download document from S3                                â”‚
â”‚     4. Run GPU-accelerated OCR                                  â”‚
â”‚     5. Claude Vision analysis                                   â”‚
â”‚     6. Upload results to S3                                     â”‚
â”‚     7. Update Supabase metadata                                 â”‚
â”‚     8. Report status to Droplet                                 â”‚
â”‚     9. Process next job                                         â”‚
â”‚    10. Auto-shutdown when queue empty                           â”‚
â”‚                                                                   â”‚
â”‚  ðŸ’¾ NO Local Storage (Stateless)                                â”‚
â”‚     â””â”€ All data goes to S3 immediately                          â”‚
â”‚                                                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â–²
                                â”‚ (Destroyed after processing)
                                â–¼
                        âš¡ Instance Lifecycle:
                        â”œâ”€ Created: When jobs queued
                        â”œâ”€ Runs: Until queue empty
                        â”œâ”€ Destroyed: After idle 5 min
                        â””â”€ Cost: Only pay for actual use
```

---

## ðŸ’¾ Cloud Storage Strategy (DigitalOcean Spaces)

### Why DigitalOcean Spaces?

- âœ… S3-compatible (works with boto3)
- âœ… $5/month for 250GB
- âœ… Same datacenter as Droplet (fast)
- âœ… CDN included (Spaces CDN)
- âœ… Easy to mount on both systems

### Alternative: AWS S3
- Cheaper for large storage
- More complex billing
- Higher egress costs

### Storage Structure

```
aseagi-documents/  (DO Spaces bucket)
â”‚
â”œâ”€â”€ raw/                          # Original uploads
â”‚   â”œâ”€â”€ 2025/
â”‚   â”‚   â”œâ”€â”€ 11/
â”‚   â”‚   â”‚   â”œâ”€â”€ 14/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ abc123.pdf
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ def456.jpg
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ ghi789.png
â”‚   â”‚
â”œâ”€â”€ processed/                    # After GPU processing
â”‚   â”œâ”€â”€ 2025/11/14/
â”‚   â”‚   â”œâ”€â”€ abc123/
â”‚   â”‚   â”‚   â”œâ”€â”€ ocr.txt
â”‚   â”‚   â”‚   â”œâ”€â”€ metadata.json
â”‚   â”‚   â”‚   â”œâ”€â”€ thumbnail.jpg
â”‚   â”‚   â”‚   â””â”€â”€ analysis.json
â”‚   â”‚
â”œâ”€â”€ cache/                        # Temporary processing
â”‚   â””â”€â”€ (auto-cleaned daily)
â”‚
â””â”€â”€ exports/                      # Generated reports
    â”œâ”€â”€ weekly-report-2025-11-14.pdf
    â””â”€â”€ case-summary-proj344.pdf
```

---

## ðŸ”„ Processing Workflow

### Scenario 1: Upload via Telegram

```
1. User sends document to Telegram bot
   â†“
2. Droplet receives webhook
   â†“
3. Droplet uploads to S3 Spaces (/raw/)
   â†“
4. Droplet creates job in Supabase:
   {
     "job_id": "uuid",
     "file_path": "s3://aseagi/raw/2025/11/14/doc.pdf",
     "status": "queued",
     "priority": "high"
   }
   â†“
5. Droplet checks: Any Vast.ai instance running?
   â”œâ”€ YES: Job will be picked up
   â””â”€ NO: Launch Vast.ai instance
   â†“
6. Vast.ai instance starts (2-3 minutes)
   â†“
7. Container runs worker script:
   - Connects to Supabase
   - Fetches jobs with status="queued"
   - Downloads from S3
   - Processes with GPU
   - Uploads results to S3
   - Updates Supabase status="completed"
   â†“
8. After 5 min idle: Instance auto-destroys
   â†“
9. User sees result in:
   - Telegram bot reply
   - Web dashboard
   - Supabase metadata
```

### Scenario 2: Bulk Upload (1000+ documents)

```
1. User uploads folder to S3 Spaces via:
   - Mac Mini â†’ rclone sync
   - Or web dashboard â†’ direct upload
   â†“
2. User clicks "Start Bulk Processing" in dashboard
   â†“
3. Droplet creates jobs for all files:
   - Scans S3 bucket
   - Creates Supabase job records
   - Estimates cost ($0.02 Ã— 1000 = $20)
   â†“
4. User confirms
   â†“
5. Droplet launches Vast.ai instance
   (selects cheapest available RTX 3080)
   â†“
6. Instance processes in parallel (8 workers)
   - ~100 docs/hour
   - 10 hours total
   - Cost: $0.40/hr Ã— 10 = $4.00
   â†“
7. Progress visible in dashboard (real-time)
   â†“
8. Instance auto-destroys when done
   â†“
9. All results in S3 + Supabase
```

### Scenario 3: View Document (No GPU needed)

```
1. User opens web dashboard
   â†“
2. Searches for "police report"
   â†“
3. Droplet queries Supabase metadata
   â†“
4. Returns matching documents
   â†“
5. User clicks to view
   â†“
6. Droplet generates signed S3 URL
   â†“
7. Browser loads document directly from S3 CDN
   (No processing, no GPU, no Vast.ai cost)
```

---

## ðŸ³ Vast.ai Docker Image

### Dockerfile

```dockerfile
# Dockerfile for Vast.ai GPU Processing
FROM nvidia/cuda:12.2.0-runtime-ubuntu22.04

ENV DEBIAN_FRONTEND=noninteractive
ENV PYTHONUNBUFFERED=1

# Install system dependencies
RUN apt-get update && apt-get install -y \
    python3.11 \
    python3-pip \
    tesseract-ocr \
    tesseract-ocr-eng \
    libtesseract-dev \
    libgl1-mesa-glx \
    libglib2.0-0 \
    curl \
    wget \
    git \
    && rm -rf /var/lib/apt/lists/*

# Install Python packages
RUN pip3 install --no-cache-dir \
    anthropic \
    supabase \
    boto3 \
    pillow \
    pytesseract \
    opencv-python-headless \
    python-dotenv \
    redis \
    requests

# Create working directory
WORKDIR /app

# Copy processing scripts
COPY scripts/ /app/scripts/
COPY worker.py /app/
COPY config.py /app/

# Set executable permissions
RUN chmod +x /app/worker.py

# Environment variables (will be set by Vast.ai)
ENV SUPABASE_URL=""
ENV SUPABASE_KEY=""
ENV ANTHROPIC_API_KEY=""
ENV S3_ENDPOINT=""
ENV S3_ACCESS_KEY=""
ENV S3_SECRET_KEY=""
ENV S3_BUCKET=""

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD python3 -c "import sys; sys.exit(0)"

# Start worker
CMD ["python3", "-u", "worker.py"]
```

### worker.py (GPU Processing Worker)

```python
#!/usr/bin/env python3
"""
Vast.ai GPU Worker
Processes documents from queue, uses GPU, writes to S3
"""

import os
import time
import logging
from pathlib import Path
from typing import Optional, Dict, Any
import boto3
from supabase import create_client
import anthropic
from PIL import Image
import pytesseract

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class VastaiWorker:
    def __init__(self):
        # Initialize clients
        self.supabase = create_client(
            os.environ['SUPABASE_URL'],
            os.environ['SUPABASE_KEY']
        )

        self.claude = anthropic.Anthropic(
            api_key=os.environ['ANTHROPIC_API_KEY']
        )

        self.s3 = boto3.client(
            's3',
            endpoint_url=os.environ['S3_ENDPOINT'],
            aws_access_key_id=os.environ['S3_ACCESS_KEY'],
            aws_secret_access_key=os.environ['S3_SECRET_KEY']
        )

        self.bucket = os.environ['S3_BUCKET']
        self.idle_count = 0
        self.max_idle = 5  # 5 minutes idle = shutdown

    def fetch_job(self) -> Optional[Dict[str, Any]]:
        """Fetch next job from Supabase"""
        try:
            result = self.supabase.table('processing_jobs')\
                .select('*')\
                .eq('status', 'queued')\
                .order('priority', desc=True)\
                .limit(1)\
                .execute()

            if result.data:
                job = result.data[0]
                # Mark as processing
                self.supabase.table('processing_jobs')\
                    .update({'status': 'processing', 'started_at': 'now()'})\
                    .eq('id', job['id'])\
                    .execute()
                return job
            return None
        except Exception as e:
            logger.error(f"Error fetching job: {e}")
            return None

    def download_from_s3(self, s3_path: str, local_path: str):
        """Download file from S3"""
        # Extract key from s3:// URL
        key = s3_path.replace(f"s3://{self.bucket}/", "")
        self.s3.download_file(self.bucket, key, local_path)

    def upload_to_s3(self, local_path: str, s3_key: str):
        """Upload file to S3"""
        self.s3.upload_file(local_path, self.bucket, s3_key)

    def process_document(self, job: Dict[str, Any]) -> Dict[str, Any]:
        """Main processing function"""
        job_id = job['id']
        s3_path = job['file_path']

        logger.info(f"Processing job {job_id}: {s3_path}")

        # Create temp directory
        temp_dir = Path(f"/tmp/job_{job_id}")
        temp_dir.mkdir(exist_ok=True)

        try:
            # Download file
            local_file = temp_dir / "input.file"
            self.download_from_s3(s3_path, str(local_file))

            # Run OCR (GPU accelerated)
            ocr_text = pytesseract.image_to_string(Image.open(local_file))

            # Claude Vision analysis
            with open(local_file, 'rb') as f:
                image_data = base64.b64encode(f.read()).decode()

            response = self.claude.messages.create(
                model="claude-opus-4-20250514",
                max_tokens=1024,
                messages=[{
                    "role": "user",
                    "content": [
                        {
                            "type": "image",
                            "source": {
                                "type": "base64",
                                "media_type": "image/jpeg",
                                "data": image_data
                            }
                        },
                        {
                            "type": "text",
                            "text": "Analyze this legal document. Extract: type, date, parties, case number, relevancy score."
                        }
                    ]
                }]
            )

            analysis = response.content[0].text

            # Save results to S3
            results_dir = temp_dir / "results"
            results_dir.mkdir(exist_ok=True)

            # OCR text
            ocr_file = results_dir / "ocr.txt"
            ocr_file.write_text(ocr_text)
            self.upload_to_s3(
                str(ocr_file),
                f"processed/{job_id}/ocr.txt"
            )

            # Analysis JSON
            analysis_file = results_dir / "analysis.json"
            analysis_file.write_text(analysis)
            self.upload_to_s3(
                str(analysis_file),
                f"processed/{job_id}/analysis.json"
            )

            # Update Supabase
            result = {
                'status': 'completed',
                'completed_at': 'now()',
                'ocr_path': f"s3://{self.bucket}/processed/{job_id}/ocr.txt",
                'analysis_path': f"s3://{self.bucket}/processed/{job_id}/analysis.json",
                'ocr_text': ocr_text[:1000],  # First 1000 chars
                'error': None
            }

            return result

        except Exception as e:
            logger.error(f"Error processing job {job_id}: {e}")
            return {
                'status': 'error',
                'error': str(e),
                'completed_at': 'now()'
            }

        finally:
            # Cleanup
            import shutil
            shutil.rmtree(temp_dir, ignore_errors=True)

    def run(self):
        """Main worker loop"""
        logger.info("ðŸš€ Vast.ai worker started")
        logger.info(f"Bucket: {self.bucket}")

        while True:
            # Fetch job
            job = self.fetch_job()

            if job:
                self.idle_count = 0

                # Process
                result = self.process_document(job)

                # Update Supabase
                self.supabase.table('processing_jobs')\
                    .update(result)\
                    .eq('id', job['id'])\
                    .execute()

                logger.info(f"âœ… Job {job['id']} completed")

            else:
                # No jobs
                self.idle_count += 1
                logger.info(f"â° Idle {self.idle_count}/{self.max_idle} minutes")

                if self.idle_count >= self.max_idle:
                    logger.info("ðŸ’¤ Max idle reached, shutting down")
                    # Signal to Vast.ai to destroy instance
                    # (Vast.ai will auto-destroy on exit)
                    break

                # Wait 1 minute
                time.sleep(60)

if __name__ == "__main__":
    worker = VastaiWorker()
    worker.run()
```

---

## ðŸŽ›ï¸ Droplet Control Plane (Flask API)

### app.py (Flask API on Droplet)

```python
#!/usr/bin/env python3
"""
ASEAGI Control Plane
Runs on DigitalOcean Droplet
Manages Vast.ai instances and job queue
"""

from flask import Flask, request, jsonify, render_template
from supabase import create_client
import subprocess
import os
import json

app = Flask(__name__)

# Initialize Supabase
supabase = create_client(
    os.environ['SUPABASE_URL'],
    os.environ['SUPABASE_KEY']
)

VASTAI_API_KEY = os.environ['VASTAI_API_KEY']

class VastaiController:
    """Control Vast.ai instances"""

    def find_cheapest_instance(self, gpu_type="RTX 3080"):
        """Find cheapest available instance"""
        cmd = f"vastai search offers 'gpu_name={gpu_type} rentable=True' --raw"
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)

        if result.returncode == 0:
            offers = json.loads(result.stdout)
            # Sort by price
            offers.sort(key=lambda x: x['dph_total'])
            return offers[0] if offers else None
        return None

    def launch_instance(self, offer_id, docker_image="aseagi/document-processor:latest"):
        """Launch Vast.ai instance"""
        env_vars = {
            'SUPABASE_URL': os.environ['SUPABASE_URL'],
            'SUPABASE_KEY': os.environ['SUPABASE_KEY'],
            'ANTHROPIC_API_KEY': os.environ['ANTHROPIC_API_KEY'],
            'S3_ENDPOINT': os.environ['S3_ENDPOINT'],
            'S3_ACCESS_KEY': os.environ['S3_ACCESS_KEY'],
            'S3_SECRET_KEY': os.environ['S3_SECRET_KEY'],
            'S3_BUCKET': os.environ['S3_BUCKET']
        }

        env_str = " ".join([f"-e {k}={v}" for k, v in env_vars.items()])

        cmd = f"vastai create instance {offer_id} --image {docker_image} {env_str}"
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)

        if result.returncode == 0:
            return json.loads(result.stdout)
        return None

    def get_running_instances(self):
        """Get list of running instances"""
        cmd = "vastai show instances --raw"
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)

        if result.returncode == 0:
            return json.loads(result.stdout)
        return []

    def destroy_instance(self, instance_id):
        """Destroy Vast.ai instance"""
        cmd = f"vastai destroy instance {instance_id}"
        subprocess.run(cmd, shell=True)

controller = VastaiController()

@app.route('/api/jobs/create', methods=['POST'])
def create_job():
    """Create new processing job"""
    data = request.json

    job = {
        'file_path': data['file_path'],
        'priority': data.get('priority', 'normal'),
        'status': 'queued',
        'created_at': 'now()'
    }

    result = supabase.table('processing_jobs').insert(job).execute()

    # Check if we need to launch instance
    jobs_queued = supabase.table('processing_jobs')\
        .select('id')\
        .eq('status', 'queued')\
        .execute()

    running_instances = controller.get_running_instances()

    if len(jobs_queued.data) > 0 and len(running_instances) == 0:
        # Launch instance
        offer = controller.find_cheapest_instance()
        if offer:
            instance = controller.launch_instance(offer['id'])
            return jsonify({
                'job': result.data[0],
                'instance_launched': True,
                'instance_id': instance.get('new_contract')
            })

    return jsonify({'job': result.data[0], 'instance_launched': False})

@app.route('/api/instances/status', methods=['GET'])
def instances_status():
    """Get status of all instances"""
    instances = controller.get_running_instances()

    jobs_queued = supabase.table('processing_jobs')\
        .select('id')\
        .eq('status', 'queued')\
        .execute()

    jobs_processing = supabase.table('processing_jobs')\
        .select('id')\
        .eq('status', 'processing')\
        .execute()

    return jsonify({
        'instances': instances,
        'jobs_queued': len(jobs_queued.data),
        'jobs_processing': len(jobs_processing.data)
    })

@app.route('/telegram/webhook', methods=['POST'])
def telegram_webhook():
    """Handle Telegram uploads"""
    data = request.json

    # Upload to S3 Spaces
    # Create job
    # Return response

    return jsonify({'status': 'ok'})

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000)
```

---

## ðŸ“¦ Repository Structure for Droplet

```
aseagi-droplet/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â”œâ”€â”€ docker-compose.yml
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ app.py              # Flask control plane
â”‚   â”œâ”€â”€ vastai_controller.py
â”‚   â””â”€â”€ telegram_bot.py
â”‚
â”œâ”€â”€ dashboard/
â”‚   â”œâ”€â”€ streamlit_app.py    # Monitoring dashboard
â”‚   â””â”€â”€ pages/
â”‚       â”œâ”€â”€ documents.py
â”‚       â”œâ”€â”€ jobs.py
â”‚       â””â”€â”€ costs.py
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ deploy.sh
â”‚   â”œâ”€â”€ backup.sh
â”‚   â””â”€â”€ setup_spaces.sh
â”‚
â”œâ”€â”€ vastai/
â”‚   â”œâ”€â”€ Dockerfile          # For Vast.ai GPU
â”‚   â”œâ”€â”€ worker.py
â”‚   â”œâ”€â”€ config.py
â”‚   â””â”€â”€ build_and_push.sh
â”‚
â””â”€â”€ nginx/
    â””â”€â”€ aseagi.conf
```

Let me continue creating the actual implementation files...

