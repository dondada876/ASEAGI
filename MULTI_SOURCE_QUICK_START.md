# Multi-Source Document Processing - Quick Start Guide
**Get all your documents from Mac Mini, Laptop, and Google Drive into one unified system**

---

## ğŸ¯ Goal

Scan all document sources â†’ Deduplicate â†’ Process â†’ Store in Supabase â†’ Generate embeddings â†’ Enable semantic search

**No duplicates. No manual tracking. Fully automated.**

---

## ğŸ“‹ Prerequisites

```bash
# Install required libraries
pip install supabase openai hashlib striprtf python-docx PyPDF2
```

**API Keys Needed:**
- âœ… Supabase (already configured)
- â³ OpenAI (add to secrets.toml)
- ğŸ”® Google Drive API (optional, for cloud scanning)
- ğŸ”® Twelve Labs (future, for video/audio)

---

## ğŸš€ Quick Start (5 Steps)

### **Step 1: Set Up Master Registry in Supabase**

```bash
# Run this SQL in Supabase SQL Editor
# Copy from: master_document_registry_schema.sql

# Or use this command to view:
cat master_document_registry_schema.sql
```

**What it creates:**
- `master_document_registry` table
- Deduplication using MD5 hash
- 8 views for common queries
- Helper functions

---

### **Step 2: Scan Your Current Machine (Laptop)**

```bash
cd /home/user/ASEAGI

# Run the scanner
python3 multi_source_scanner.py

# It will scan:
# - /home/user/ASEAGI
# - /home/user/Documents
# - /home/user/Downloads
# - ~/Documents
# - ~/Downloads

# Output: laptop_document_registry.json
```

**Expected output:**
```
ğŸ” SCANNING: /home/user/ASEAGI
ğŸ“‚ SOURCE: laptop

âœ… NEW: document1.pdf (2.5 MB)
âœ… NEW: report.docx (156 KB)
âš ï¸  DUPLICATE: document1.pdf (already scanned)

ğŸ“Š SCAN COMPLETE: laptop
âœ… New documents: 150
âš ï¸  Duplicates found: 5
ğŸ“ Total unique: 150
```

---

### **Step 3: Scan Mac Mini (When Home)**

```bash
# On Mac Mini, run the same command:
python3 multi_source_scanner.py

# Copy the output file to laptop:
# mac_mini_document_registry.json
```

---

### **Step 4: (Optional) Scan Google Drive**

```bash
# Set up Google Drive API credentials
# See: https://developers.google.com/drive/api/quickstart/python

# Run scanner
python3 google_drive_scanner.py

# Output: gdrive_document_registry.json
```

---

### **Step 5: Consolidate All Sources**

```bash
# Merge all registries into Supabase
python3 consolidate_registries.py

# It will:
# 1. Load all registry JSON files
# 2. Detect duplicates across sources (same file in multiple locations)
# 3. Upload to master_document_registry in Supabase
# 4. Generate consolidation report
```

**Expected output:**
```
CONSOLIDATING REGISTRIES
âœ… Loaded: laptop_document_registry.json
   Source: laptop
   Documents: 150

âœ… Loaded: mac_mini_document_registry.json
   Source: mac_mini
   Documents: 280

âš ï¸  DUPLICATE: project_plan.docx
   Hash: abc123...
   Now in 2 locations

CONSOLIDATION COMPLETE
Total files scanned: 430
Unique documents: 375
Duplicates across sources: 55

UPLOADING TO SUPABASE
âœ… NEW: document1.pdf
âœ… NEW: report.docx
âœï¸  UPDATED: project_plan.docx (now in 2 locations)
```

---

## ğŸ“Š Check Your Results

### In Supabase SQL Editor:

```sql
-- Total documents registered
SELECT * FROM processing_stats;

-- Documents by source
SELECT * FROM documents_by_source;

-- Duplicates across sources
SELECT * FROM document_duplicates;

-- Processing queue (pending documents)
SELECT * FROM documents_to_process LIMIT 10;
```

---

## ğŸ”„ Process All Documents

Once consolidated, process all documents:

```bash
# Add OpenAI key to secrets.toml first!
nano .streamlit/secrets.toml
# Add: OPENAI_API_KEY = "sk-proj-..."

# Extract text from all documents
python3 document_extractor.py

# Upload to Supabase with embeddings
python3 document_repository_to_supabase.py
```

---

## ğŸ” Search Your Documents

### Keyword Search (Fast):
```sql
-- Find documents mentioning "custody"
SELECT * FROM search_documents('custody', 10);
```

### Semantic Search (Smart):
```python
# Find documents by meaning
from document_repository_to_supabase import DocumentRepositoryUploader

uploader = DocumentRepositoryUploader()
results = uploader.search_semantic("documents about protective orders")

for result in results:
    print(f"{result['file_name']}: {result['similarity']}")
```

---

## ğŸ“ File Locations

```
/home/user/ASEAGI/
â”œâ”€â”€ MULTI_SOURCE_DATA_STRATEGY.md          # Complete strategy doc
â”œâ”€â”€ MULTI_SOURCE_QUICK_START.md            # This guide
â”œâ”€â”€ master_document_registry_schema.sql    # Database schema
â”œâ”€â”€ multi_source_scanner.py                # Local scanner
â”œâ”€â”€ consolidate_registries.py              # Registry merger
â”œâ”€â”€ document_extractor.py                  # Text extraction
â”œâ”€â”€ document_repository_to_supabase.py     # Upload & embeddings
â”‚
â”œâ”€â”€ laptop_document_registry.json          # Generated by scanner
â”œâ”€â”€ mac_mini_document_registry.json        # Copy from Mac Mini
â”œâ”€â”€ gdrive_document_registry.json          # Generated by GDrive scanner
â””â”€â”€ consolidation_report.json              # Generated by consolidator
```

---

## ğŸ¯ Common Scenarios

### Scenario 1: "I just want to scan my laptop"
```bash
python3 multi_source_scanner.py
# Done! Review laptop_document_registry.json
```

### Scenario 2: "I want all sources in Supabase"
```bash
# 1. Scan laptop
python3 multi_source_scanner.py

# 2. Scan Mac Mini (when home)
python3 multi_source_scanner.py

# 3. Consolidate
python3 consolidate_registries.py
```

### Scenario 3: "I found duplicates, how do I clean them up?"
```sql
-- View duplicates
SELECT * FROM document_duplicates;

-- Documents are NOT duplicated in database
-- Same file = 1 row with multiple source_locations
-- No cleanup needed!
```

### Scenario 4: "I added new documents, how do I rescan?"
```bash
# Just run scanner again
python3 multi_source_scanner.py

# Then consolidate
python3 consolidate_registries.py

# It will only add NEW documents
# Existing documents will be updated with new location if found
```

---

## ğŸ’¡ How Deduplication Works

```python
# File on laptop: /home/user/Documents/report.pdf
# File on Mac Mini: /Users/don/Documents/report.pdf
# File in Google Drive: My Drive/Documents/report.pdf

# ALL THREE = Same MD5 hash = 1 row in database:

{
  "file_hash": "abc123...",
  "file_name": "report.pdf",
  "source_locations": [
    {"source": "laptop", "path": "/home/user/Documents/report.pdf"},
    {"source": "mac_mini", "path": "/Users/don/Documents/report.pdf"},
    {"source": "gdrive", "path": "https://drive.google.com/file/d/..."}
  ],
  "primary_location": "laptop",
  "is_cloud_backed_up": true
}
```

**Benefits:**
- âœ… No duplicate processing (same file processed once)
- âœ… Track all locations (know where file exists)
- âœ… Know if backed up (is_cloud_backed_up flag)
- âœ… Choose preferred location (primary_location)

---

## ğŸ”® Future Enhancements

### Phase 2: Graph RAG (Neo4j)
```bash
# Build knowledge graph of document relationships
# "Which documents mention both Person A and Statute B?"
# "Show me the network of related court filings"
```

### Phase 3: Video/Audio (Twelve Labs)
```bash
# Transcribe video depositions
# Analyze audio recordings
# Search across text AND multimedia
```

### Phase 4: Multi-Device Access
```bash
# FastAPI REST API
# Mobile app
# Web dashboard
```

---

## ğŸ†˜ Troubleshooting

### "Scanner found 0 documents"
**Check:**
- Paths exist: `ls /home/user/ASEAGI`
- File extensions correct (default: .rtf, .doc, .docx, .pdf, .txt, .md)
- Not in excluded directories (node_modules, .git, etc.)

### "Consolidation failed: table does not exist"
**Solution:**
```bash
# Run the SQL schema first
cat master_document_registry_schema.sql
# Copy and paste into Supabase SQL Editor
```

### "Duplicate hash error"
**This is normal!**
- Same file in different locations = expected
- Consolidator handles this automatically
- Result: 1 row with multiple source_locations

### "Python module not found"
```bash
pip install supabase openai striprtf python-docx PyPDF2
```

---

## ğŸ“Š Status Dashboard

Check your consolidation status:

```sql
-- Overall stats
SELECT * FROM processing_stats;

-- Sample output:
-- total_documents: 375
-- pending: 375
-- complete: 0
-- cloud_backed_up: 120
-- total_size_gb: 2.5
```

---

## âœ… Success Checklist

- [ ] Master registry schema deployed to Supabase
- [ ] Laptop scanned (laptop_document_registry.json created)
- [ ] Mac Mini scanned (when home)
- [ ] Google Drive scanned (optional)
- [ ] All registries consolidated into Supabase
- [ ] OpenAI key added to secrets.toml
- [ ] Documents processed with document_extractor.py
- [ ] Embeddings generated
- [ ] Semantic search working

---

## ğŸ‰ What You Get

After completing all steps:

âœ… **Unified Document Registry**
- All documents from all sources in one place
- No duplicates
- Know where each file exists

âœ… **Smart Deduplication**
- Same file = processed once
- Tracked in all locations
- Automatic backup detection

âœ… **Semantic Search**
- "Find documents about custody arrangements"
- "Show me evidence of protective order violations"
- Natural language queries

âœ… **Ready for Scale**
- Add more sources anytime
- Process 1 document or 10,000
- Architecture supports future enhancements (video, graph RAG, etc.)

---

**For Ashe. For Justice. For All Children.** ğŸ›¡ï¸
